{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "simple_NN_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATA REQUIREMENTS\n",
        "\n",
        "1. Create a folder called data\n",
        "2. Inside the folder 'data' create two sub folders name one folder as deer and other folder as dog. -- we need two diffrent object you can choose Dog vs Donut, lotus vs lilly etc. the folder name should be same as the class\n",
        "\n",
        "3. collect ~20 images of dogs and save in dog folder. Rename them uniformly\n",
        "\n",
        "4. collect ~20 images of deer and save in deer folder.\n",
        "rename them uniformly.\n",
        "\n",
        "-- No. of images has no restrictions. \n",
        "\n",
        "5. Zip the top folder and upload to your google drive"
      ],
      "metadata": {
        "id": "Cm9TBqy6lTi5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DDssaqxueU_"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHVdDdgOqoXI"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11p0UV8p7yEy"
      },
      "source": [
        "import shutil\n",
        "if(os.path.isdir(\"data\")):   # \"data\" name should be same as in folder where images are saved in the pc and loaded to drive\n",
        "  shutil.rmtree(\"data\", ignore_errors=False, onerror=None)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Google drive"
      ],
      "metadata": {
        "id": "K-3IQzyU1GEB"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6jUrj4YJwXc",
        "outputId": "f606bcc8-4271-42c3-9287-a2d2b1cd786f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive  #mounting google drive to co lab for accessing dataset colleted in previous step \"data\"\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# copy the path and paste in the code below\n",
        "\n",
        "-- change the path and folder name based on the location of your data in the drive (if necessary)"
      ],
      "metadata": {
        "id": "E5k-pxOU1SxU"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUBrC4S8KG5A"
      },
      "source": [
        "root_path = '/content/gdrive/MyDrive/data.zip'  #change dir to your project folder"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0JZZLTGUwca"
      },
      "source": [
        "# Uncompress the feature images and labels csv\n",
        "def uncompress_features_labels(dir,name):\n",
        "    if(os.path.isdir(name)):\n",
        "        print('Data extracted')\n",
        "    else:\n",
        "        with ZipFile(dir) as zipf:\n",
        "            zipf.extractall(name)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y6JrDVRU0z-"
      },
      "source": [
        "uncompress_features_labels(root_path,'data')  #once uncompressed on the left tab u must be able to see both the folders inside"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-QucE9MYNm7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c31c9e4-533b-4901-ba84-048800b49741"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaG5AYBsXxk9"
      },
      "source": [
        "DATADIR = \"data/data\"\n",
        "CATEGORIES = [\"deer\", \"dog\"] # folder name as inside the data folder # os.listdir(\"data/data/\") can be used\n",
        "IMG_SIZE = 224"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdQubKThPLzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed69491-8350-40e8-e13c-f8a990eb7a43"
      },
      "source": [
        "os.listdir(\"data/data/deer\")  #to view the list of images"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['d19.jpg',\n",
              " 'd12.jpg',\n",
              " 'd17.jpg',\n",
              " 'd18.jpg',\n",
              " 'd2.jpg',\n",
              " 'd3.jpg',\n",
              " 'd9.jpg',\n",
              " 'd6.jpg',\n",
              " 'd16.jpg',\n",
              " 'd7.jpg',\n",
              " 'd15.jpg',\n",
              " 'deer_1.jpg',\n",
              " 'd10.jpg',\n",
              " 'd20.jpg',\n",
              " 'd11.jpg',\n",
              " 'd4.jpg',\n",
              " 'd13.jpg',\n",
              " 'd5.jpg',\n",
              " 'd8.jpg',\n",
              " 'd14.jpg']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA-jpCgPY-E9",
        "outputId": "e0a85d5b-21a8-48e9-c0d3-56043f61ca71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_data = []\n",
        "\n",
        "def create_training_data():\n",
        "    for category in CATEGORIES:  \n",
        "\n",
        "        path = os.path.join(DATADIR,category) # os.path look in the path data/data/mime\n",
        "        class_num = CATEGORIES.index(category) # 0  catergory deer =0 and dog in 1\n",
        "\n",
        "        for img in tqdm(os.listdir(path)): #all images in the directory will be listed\n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path,img) ,cv2.COLOR_BGR2RGB) \n",
        "                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), interpolation=cv2.INTER_CUBIC) \n",
        "                training_data.append([new_array, class_num])\n",
        "            except Exception as e:  \n",
        "                pass\n",
        "              \n",
        "create_training_data() # output will give number of images succesfully loaded"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20/20 [00:00<00:00, 240.53it/s]\n",
            "100%|██████████| 20/20 [00:00<00:00, 698.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OxtSEoxZwma",
        "outputId": "9cb66737-5273-4671-c992-9b814d26e26d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(len(training_data))\n",
        "\n",
        "# center_image = cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB)\n",
        "# res = cv2.resize(center_image, dsize=(224, 224), interpolation=cv2.INTER_CUBIC)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbBfUFJIZ7uc",
        "outputId": "a2125654-4270-4233-cf5f-9cbad480d378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(training_data[0][0].shape) # input image size\n",
        "print(training_data[0][1])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(224, 224, 3)\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MszxghYKcqnu"
      },
      "source": [
        "## Shuffle the data or else the order of the image may affect the perfomance of the network\n",
        "\n",
        "Basic methods for training\n",
        "To train a model with this dataset you will want the data:\n",
        "\n",
        "* To be well shuffled.\n",
        "* To be batched.\n",
        "* Batches to be available as soon as possible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWvaG4YUcqAB"
      },
      "source": [
        "import random\n",
        "random.shuffle(training_data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmI1EBredGeG",
        "outputId": "d01b8dc9-0bad-4610-85aa-38ce4d8455b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for sample in training_data[:10]:\n",
        "    print(sample[1])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "0\n",
            "0\n",
            "1\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq2hN3ZYswNK"
      },
      "source": [
        "images = []\n",
        "labels = []\n",
        "\n",
        "for features,label in training_data:\n",
        "    images.append(features)\n",
        "    labels.append(label)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCpdXHfofWpD"
      },
      "source": [
        "X = np.array(images) # input\n",
        "X = X/255\n",
        "y = np.array(labels) #output"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9AhsAYz3iOQ"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v1jFcTpswQ-",
        "outputId": "9fd37398-706c-4dd4-da45-a58126dc8ec2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(X_train.shape) # 32 images for training size 224x224x3\n",
        "print(X_test.shape)\n",
        "print(y_train.shape) # \n",
        "print(y_test.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 224, 224, 3)\n",
            "(8, 224, 224, 3)\n",
            "(32,)\n",
            "(8,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNXiMmxQ1EX-",
        "outputId": "abdfebdc-1731-4ac5-858b-dd4976d449cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(y_train)\n",
        "print(y_test)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 1 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 0 0 0 1]\n",
            "[1 1 1 1 1 0 1 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRM4d_vut6da"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Activation, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # for data augmentation\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau # to know at which epoch best accuracy acheived\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline  \n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArhITKyqt6T1"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3,3), strides=(1,1), padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3), data_format='channels_last'))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.25)) # to avoid overfitting\n",
        "\n",
        "model.add(Dense(84))\n",
        "model.add(Activation(\"relu\"))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(2, activation='softmax')) # 2 is the number of classes  or len(CLASS_NAMES) can be used"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-zH2sFQt6gy"
      },
      "source": [
        "model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF1LEwNuEle7",
        "outputId": "ab12db05-9025-458c-ed7d-cb5a95848f2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# batch_size = 32\n",
        "# datagen = ImageDataGenerator()\n",
        "# datagen.fit(X_train)\n",
        "# X_batch, y_batch = datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "# model.fit_generator(datagen, samples_per_epoch=len(train), epochs=epochs)\n",
        "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1/1 [==============================] - 10s 10s/step - loss: 0.7164 - accuracy: 0.2812 - val_loss: 10.5481 - val_accuracy: 0.1250\n",
            "Epoch 2/20\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 4.5048 - accuracy: 0.6250 - val_loss: 1.8938 - val_accuracy: 0.8750\n",
            "Epoch 3/20\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 8.3965 - accuracy: 0.4062 - val_loss: 0.6033 - val_accuracy: 0.8750\n",
            "Epoch 4/20\n",
            "1/1 [==============================] - 0s 139ms/step - loss: 4.8128 - accuracy: 0.3125 - val_loss: 2.6898 - val_accuracy: 0.1250\n",
            "Epoch 5/20\n",
            "1/1 [==============================] - 0s 151ms/step - loss: 1.9869 - accuracy: 0.5312 - val_loss: 3.8916 - val_accuracy: 0.1250\n",
            "Epoch 6/20\n",
            "1/1 [==============================] - 0s 140ms/step - loss: 1.8999 - accuracy: 0.5312 - val_loss: 2.8053 - val_accuracy: 0.1250\n",
            "Epoch 7/20\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 1.4299 - accuracy: 0.6562 - val_loss: 1.3931 - val_accuracy: 0.1250\n",
            "Epoch 8/20\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.7112 - accuracy: 0.5938 - val_loss: 0.7243 - val_accuracy: 0.3750\n",
            "Epoch 9/20\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.5510 - accuracy: 0.7812 - val_loss: 0.4805 - val_accuracy: 0.8750\n",
            "Epoch 10/20\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5210 - accuracy: 0.7500 - val_loss: 0.6863 - val_accuracy: 0.3750\n",
            "Epoch 11/20\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.3350 - accuracy: 0.9062 - val_loss: 2.0672 - val_accuracy: 0.1250\n",
            "Epoch 12/20\n",
            "1/1 [==============================] - 0s 142ms/step - loss: 0.5121 - accuracy: 0.6875 - val_loss: 0.7159 - val_accuracy: 0.3750\n",
            "Epoch 13/20\n",
            "1/1 [==============================] - 0s 141ms/step - loss: 0.2779 - accuracy: 0.8750 - val_loss: 0.3043 - val_accuracy: 1.0000\n",
            "Epoch 14/20\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.3278 - accuracy: 0.8125 - val_loss: 0.3122 - val_accuracy: 1.0000\n",
            "Epoch 15/20\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.2906 - accuracy: 0.9062 - val_loss: 0.6341 - val_accuracy: 0.3750\n",
            "Epoch 16/20\n",
            "1/1 [==============================] - 0s 145ms/step - loss: 0.2154 - accuracy: 0.9062 - val_loss: 1.7903 - val_accuracy: 0.1250\n",
            "Epoch 17/20\n",
            "1/1 [==============================] - 0s 144ms/step - loss: 0.5085 - accuracy: 0.7500 - val_loss: 0.5978 - val_accuracy: 0.5000\n",
            "Epoch 18/20\n",
            "1/1 [==============================] - 0s 143ms/step - loss: 0.1672 - accuracy: 0.9375 - val_loss: 0.3033 - val_accuracy: 1.0000\n",
            "Epoch 19/20\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.1684 - accuracy: 0.9375 - val_loss: 0.2745 - val_accuracy: 1.0000\n",
            "Epoch 20/20\n",
            "1/1 [==============================] - 0s 138ms/step - loss: 0.1652 - accuracy: 0.9375 - val_loss: 0.3097 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce444308d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnZYFMkyRDbJ",
        "outputId": "994ffed6-a08e-409a-b245-2e890374b37b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "dataAugmentaion = ImageDataGenerator(rotation_range = 30, zoom_range = 0.20, \n",
        "fill_mode = \"nearest\", shear_range = 0.20, horizontal_flip = True, \n",
        "width_shift_range = 0.1, height_shift_range = 0.1)\n",
        "\n",
        "# training the model\n",
        "model.fit_generator(dataAugmentaion.flow(X_train, y_train, batch_size = 32),\n",
        " validation_data = (X_test, y_test), steps_per_epoch = X_train.shape[0] // 32,\n",
        " epochs = 40)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "1/1 [==============================] - 1s 864ms/step - loss: 0.4377 - accuracy: 0.7812 - val_loss: 0.5999 - val_accuracy: 0.6250\n",
            "Epoch 2/40\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 0.3884 - accuracy: 0.7812 - val_loss: 1.4577 - val_accuracy: 0.1250\n",
            "Epoch 3/40\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.5829 - accuracy: 0.6250 - val_loss: 0.7721 - val_accuracy: 0.3750\n",
            "Epoch 4/40\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3892 - accuracy: 0.8438 - val_loss: 0.3928 - val_accuracy: 0.8750\n",
            "Epoch 5/40\n",
            "1/1 [==============================] - 1s 965ms/step - loss: 0.3386 - accuracy: 0.8438 - val_loss: 0.3003 - val_accuracy: 0.8750\n",
            "Epoch 6/40\n",
            "1/1 [==============================] - 1s 963ms/step - loss: 0.3532 - accuracy: 0.8125 - val_loss: 0.3751 - val_accuracy: 0.8750\n",
            "Epoch 7/40\n",
            "1/1 [==============================] - 1s 1s/step - loss: 0.3109 - accuracy: 0.9062 - val_loss: 0.5095 - val_accuracy: 0.8750\n",
            "Epoch 8/40\n",
            "1/1 [==============================] - 1s 590ms/step - loss: 0.3332 - accuracy: 0.9062 - val_loss: 0.4028 - val_accuracy: 0.8750\n",
            "Epoch 9/40\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 0.3404 - accuracy: 0.8438 - val_loss: 0.3886 - val_accuracy: 0.8750\n",
            "Epoch 10/40\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2537 - accuracy: 0.9062 - val_loss: 0.5763 - val_accuracy: 0.8750\n",
            "Epoch 11/40\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.3099 - accuracy: 0.9375 - val_loss: 0.4827 - val_accuracy: 0.8750\n",
            "Epoch 12/40\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.2592 - accuracy: 0.9062 - val_loss: 0.2237 - val_accuracy: 0.8750\n",
            "Epoch 13/40\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 0.2159 - accuracy: 0.9062 - val_loss: 0.2403 - val_accuracy: 0.8750\n",
            "Epoch 14/40\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.2415 - accuracy: 0.9062 - val_loss: 0.3356 - val_accuracy: 0.8750\n",
            "Epoch 15/40\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 0.2795 - accuracy: 0.9062 - val_loss: 0.5090 - val_accuracy: 0.8750\n",
            "Epoch 16/40\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.3438 - accuracy: 0.8125 - val_loss: 0.3500 - val_accuracy: 0.8750\n",
            "Epoch 17/40\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.2088 - accuracy: 0.9062 - val_loss: 0.4046 - val_accuracy: 0.8750\n",
            "Epoch 18/40\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 0.1867 - accuracy: 0.9375 - val_loss: 0.7384 - val_accuracy: 0.7500\n",
            "Epoch 19/40\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 0.3537 - accuracy: 0.8438 - val_loss: 0.2664 - val_accuracy: 0.8750\n",
            "Epoch 20/40\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.1355 - accuracy: 0.9375 - val_loss: 0.1794 - val_accuracy: 0.8750\n",
            "Epoch 21/40\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.2655 - accuracy: 0.8750 - val_loss: 0.2627 - val_accuracy: 0.8750\n",
            "Epoch 22/40\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 0.1717 - accuracy: 0.9375 - val_loss: 0.4675 - val_accuracy: 0.8750\n",
            "Epoch 23/40\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 0.2723 - accuracy: 0.8438 - val_loss: 0.8030 - val_accuracy: 0.6250\n",
            "Epoch 24/40\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.1995 - accuracy: 0.9375 - val_loss: 0.7078 - val_accuracy: 0.6250\n",
            "Epoch 25/40\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.3001 - accuracy: 0.8750 - val_loss: 0.2475 - val_accuracy: 0.8750\n",
            "Epoch 26/40\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.1436 - accuracy: 0.9688 - val_loss: 0.0982 - val_accuracy: 1.0000\n",
            "Epoch 27/40\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.2577 - accuracy: 0.8750 - val_loss: 0.2130 - val_accuracy: 0.8750\n",
            "Epoch 28/40\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.1831 - accuracy: 0.9375 - val_loss: 1.5711 - val_accuracy: 0.5000\n",
            "Epoch 29/40\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.1914 - accuracy: 0.9375 - val_loss: 2.8678 - val_accuracy: 0.3750\n",
            "Epoch 30/40\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 0.3465 - accuracy: 0.8438 - val_loss: 0.1445 - val_accuracy: 0.8750\n",
            "Epoch 31/40\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 0.1543 - accuracy: 0.9375 - val_loss: 0.0587 - val_accuracy: 1.0000\n",
            "Epoch 32/40\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 0.3011 - accuracy: 0.8438 - val_loss: 0.0923 - val_accuracy: 1.0000\n",
            "Epoch 33/40\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 0.2365 - accuracy: 0.9062 - val_loss: 0.2150 - val_accuracy: 0.8750\n",
            "Epoch 34/40\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.1449 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.7500\n",
            "Epoch 35/40\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.2423 - accuracy: 0.8750 - val_loss: 1.0384 - val_accuracy: 0.5000\n",
            "Epoch 36/40\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2755 - accuracy: 0.8125 - val_loss: 0.9688 - val_accuracy: 0.5000\n",
            "Epoch 37/40\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 0.2387 - accuracy: 0.8438 - val_loss: 0.5692 - val_accuracy: 0.7500\n",
            "Epoch 38/40\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.2260 - accuracy: 0.9062 - val_loss: 0.2907 - val_accuracy: 0.8750\n",
            "Epoch 39/40\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.1301 - accuracy: 0.9688 - val_loss: 0.1728 - val_accuracy: 0.8750\n",
            "Epoch 40/40\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 0.1926 - accuracy: 0.8750 - val_loss: 0.1375 - val_accuracy: 0.8750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fce4754bcd0>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ59rVg1b59c"
      },
      "source": [
        "predict_test = np.argmax(model.predict(X_test), axis=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xH_gLL8L3iC"
      },
      "source": [
        "CLASS_NAMES = np.array(CATEGORIES)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS7GtvWobRGj",
        "outputId": "bc6f4956-4b82-4f08-8cbf-201d797981e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(y_test, predict_test))\n",
        "print('Classification Report')\n",
        "print(classification_report(y_test, predict_test, target_names=CLASS_NAMES))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "[[1 0]\n",
            " [1 6]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        deer       0.50      1.00      0.67         1\n",
            "         dog       1.00      0.86      0.92         7\n",
            "\n",
            "    accuracy                           0.88         8\n",
            "   macro avg       0.75      0.93      0.79         8\n",
            "weighted avg       0.94      0.88      0.89         8\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTMSvqzac_4U"
      },
      "source": [
        "# con_mat_df = pd.DataFrame(confusion_matrix(y_test, predict_test), index = CLASS_NAMES, columns = CLASS_NAMES)\n",
        "\n",
        "con_mat_df = pd.DataFrame(confusion_matrix(y_test, predict_test), index = CLASS_NAMES, columns = CLASS_NAMES)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy7PDDx7c4le",
        "outputId": "3dc01241-91a1-4ce9-a512-b0285a118b64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        }
      },
      "source": [
        "figure = plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgwoa2ay6JqC",
        "outputId": "fe6133dd-df07-47a6-e4c4-8d7f021f1856",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predict_train = np.argmax(model.predict(X_train), axis=-1)\n",
        "predict_train\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn2UUkYK6JoA",
        "outputId": "73a37d03-8f4b-4183-fb81-65f94e8a13a8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8EhUVMkMFC3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsGqj2AobYNd"
      },
      "source": [
        "# Save the model\n",
        "model.save('dogdeer_model.h5')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxrVfPpD-P9r"
      },
      "source": [
        "# Recreate the exact same model purely from the file\n",
        "# new_model = keras.models.load_model('path_to_my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVZUid-K-fB3",
        "outputId": "c851e600-020b-4207-daca-5644877ce5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.save('dogdeer_tf_Serve', save_format='tf')\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: dogdeer_tf_Serve/assets\n"
          ]
        }
      ]
    }
  ]
}